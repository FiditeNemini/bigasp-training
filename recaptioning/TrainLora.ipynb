{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "from pathlib import Path\n",
    "from transformers.utils import can_return_loss, infer_framework\n",
    "import inspect\n",
    "import datasets\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\"./lora_model_ign2o4dn\", device_map=0, torch_dtype=torch.bfloat16)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./lora_model_ign2o4dn\", fast=True)\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"fancyfeast/add_source_lora_model\", private=True)\n",
    "model.push_to_hub(\"fancyfeast/add_source_lora_model\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the PROMPT to the system message, so it's baked into the model making it easier to use and harder to give the wrong prompt (if it changes between versions)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./lora_model\", fast=True)\n",
    "# assert isinstance(tokenizer.chat_template, str)\n",
    "# tokenizer.chat_template = tokenizer.chat_template.replace(\"{{- system_message }}\", '{{- ' + json.dumps(PROMPT) + ' }}')\n",
    "\n",
    "# convo = [\n",
    "# \t{\"role\": \"user\", \"content\": \"A photograph\"},\n",
    "# ]\n",
    "\n",
    "# foo = tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = True)\n",
    "# assert isinstance(foo, str)\n",
    "# print(foo)\n",
    "# input_ids2 = tokenizer.encode(foo, return_tensors = \"pt\", truncation=False, max_length = 1024, add_special_tokens=False).cuda()\n",
    "# print(input_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the LORA\n",
    "model = model.merge_and_unload(progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to hub and save locally\n",
    "#model.save_pretrained(\"big-asp-caption-watermark-rephraser-hf\")\n",
    "#tokenizer.save_pretrained(\"big-asp-caption-watermark-rephraser-hf\")\n",
    "\n",
    "model.push_to_hub(\"fancyfeast/big-asp-caption-add-source-rephraser\", private=True)\n",
    "tokenizer.push_to_hub(\"fancyfeast/big-asp-caption-add-source-rephraser\", private=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to gguf\n",
    "# $CONDA_PREFIX/lib/python3.11/site-packages/bin/convert_hf_to_gguf.py --outfile big-asp-captioner-rephraser.gguf --outtype bf16 --model-name big-asp-caption-rephraser big-asp-caption-rephraser-hf\n",
    "# Had to use docker, the script was not working on my machine\n",
    "# docker run -v (pwd)/recaptioning:/foo --rm -it ghcr.io/ggerganov/llama.cpp:full --convert --outfile /foo/big-asp-caption-rephraser.gguf --outtype bf16 --model-name big-asp-caption-rephraser /foo/big-asp-caption-rephraser-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_tokenizer = get_chat_template(\n",
    "\ttokenizer,\n",
    "\tchat_template = \"llama-3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"Please edit the user's provided image descriptions following the guidelines below:\n",
    "1. The edits should be minimal and not affect the details or accuracy of the description.\n",
    "2. Remove any mention of the image's resolution, but don't remove information about the image's quality.\n",
    "3. Edit out any self-referential language. For example: \"this is a digital painting\" -> \"a digital painting\", \"In this photo a woman stands\" -> \"photo of a woman standing\", etc.\n",
    "4. Randomly swap in informal synonyms for things like \"penis\", \"vulva\", etc.\n",
    "5. Do not modify anything in quotes that are describing text in the image.\n",
    "6. Randomly swap the word \"photograph\" to \"photo\".\n",
    "7. Remove any duplicates from the description if the description repeats itself.\n",
    "8. When you make edits, make sure to maintain the original meaning of the sentence, and minimize the number of changes.\n",
    "9. Only update the grammer if necessary. Do NOT fix any grammar mistakes or oddness that were in the original description. Some of them may be MidJourney prompts or lists of tags.\n",
    "\n",
    "Respond with only the edited image description.\n",
    "\"\"\"\n",
    "\n",
    "CAPTION = \"\"\n",
    "\n",
    "convo = [\n",
    "\t{\"role\": \"system\", \"content\": PROMPT},\n",
    "\t{\"role\": \"user\", \"content\": CAPTION},\n",
    "]\n",
    "\n",
    "foo = tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = True)\n",
    "assert isinstance(foo, str)\n",
    "print(foo)\n",
    "input_ids = tokenizer.encode(foo, return_tensors = \"pt\", truncation=False, max_length = 1024, add_special_tokens=False).cuda()\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\tgenerate_ids = model.generate(input_ids, max_new_tokens=512, suppress_tokens=None)\n",
    "\tgenerate_ids = generate_ids[:, input_ids.shape[1]:]\n",
    "\tif generate_ids[0][-1] == tokenizer.eos_token_id or generate_ids[0][-1] == tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"):\n",
    "\t\tgenerate_ids = generate_ids[:, :-1]\n",
    "\t\n",
    "\tprint(f\"Generated {generate_ids.shape[1]} tokens\")\n",
    "\n",
    "\tcaption = tokenizer.batch_decode(generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\n",
    "\tprint(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmpenv5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
