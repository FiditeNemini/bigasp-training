{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import QualityClassifier\n",
    "import sqlite3\n",
    "import random\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect('../data/clip-embeddings.sqlite3') as conn:\n",
    "\tcur = conn.cursor()\n",
    "\tcur.execute('SELECT path, score FROM images WHERE score IS NOT NULL')\n",
    "\tpaths = cur.fetchall()\n",
    "\n",
    "random_paths = random.sample(paths, 1024)\n",
    "\n",
    "bins = [[] for _ in range(10)]\n",
    "\n",
    "for path, score in random_paths:\n",
    "\tbins[score].append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bin sizes:\")\n",
    "print(\", \".join(f\"{len(bin)}\" for bin in bins))\n",
    "\n",
    "def img_html(path):\n",
    "\timage = Image.open(path)\n",
    "\tscale = 512 / max(image.size)\n",
    "\timage = image.resize((int(image.width * scale), int(image.height * scale)))\n",
    "\timage_base64 = io.BytesIO()\n",
    "\timage.save(image_base64, format='WebP', quality=80)\n",
    "\timage_base64 = base64.b64encode(image_base64.getvalue()).decode('utf-8')\n",
    "\treturn f'<img src=\"data:image/webp;base64,{image_base64}\" width=\"512\" style=\"margin: 5px;\">'\n",
    "\n",
    "html = \"<table>\"\n",
    "\n",
    "for bin_number, image_paths in enumerate(bins):\n",
    "\tsampled_images = random.sample(image_paths, 5)\n",
    "\n",
    "\trow_html = f\"<tr><td>{bin_number}</td>\"\n",
    "\tfor path in sampled_images:\n",
    "\t\trow_html += f\"<td>{img_html(path)}</td>\"\n",
    "\trow_html += \"</tr>\"\n",
    "\n",
    "\thtml += row_html\n",
    "\n",
    "html += \"</table>\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QualityClassifier(768, 0.0)\n",
    "model.load_state_dict(torch.load('classifier.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect('../data/clip-embeddings.sqlite3') as conn:\n",
    "\tcursor = conn.cursor()\n",
    "\tcursor.execute('SELECT path FROM images WHERE embedding IS NOT NULL')\n",
    "\tall_paths = [row[0] for row in cursor.fetchall()]\n",
    "\trandom_paths = random.sample(all_paths, 128)\n",
    "\n",
    "\trandom_embeddings = []\n",
    "\tfor path in random_paths:\n",
    "\t\tcursor.execute('SELECT embedding FROM images WHERE path = ?', (path,))\n",
    "\t\tembedding = bytes(cursor.fetchone()[0])\n",
    "\t\tembedding = torch.frombuffer(embedding, dtype=torch.float16).to(torch.float32)\n",
    "\t\trandom_embeddings.append(embedding)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmpenv5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
