{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Batch\n",
    "Check and make sure what's coming out of the data loading pipeline is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from data import ImageDatasetPrecoded, gen_buckets, AspectBucketSampler\n",
    "from transformers import CLIPTokenizer\n",
    "from diffusers import AutoencoderKL\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torchvision.transforms.functional as TVF\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b94cb8d56d74bad99a9fbbbb8ef33da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6714713it [00:03, 1812288.43it/s]\n",
      "2048it [00:00, 1516852.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect ratio buckets:\n",
      "(72, 208), 1: 1854\n",
      "(72, 208), 2: 1114\n",
      "(72, 208), 3: 31\n",
      "(72, 216), 1: 2892\n",
      "(72, 216), 2: 1778\n",
      "(72, 216), 3: 48\n",
      "(80, 192), 1: 10092\n",
      "(80, 192), 2: 6237\n",
      "(80, 192), 3: 156\n",
      "(80, 200), 1: 3007\n",
      "(80, 200), 2: 1771\n",
      "(80, 200), 3: 57\n",
      "(88, 168), 1: 19090\n",
      "(88, 168), 2: 12283\n",
      "(88, 168), 3: 335\n",
      "(88, 176), 1: 7424\n",
      "(88, 176), 2: 4665\n",
      "(88, 176), 3: 115\n",
      "(88, 184), 1: 10869\n",
      "(88, 184), 2: 6947\n",
      "(88, 184), 3: 175\n",
      "(96, 160), 1: 35773\n",
      "(96, 160), 2: 22332\n",
      "(96, 160), 3: 623\n",
      "(96, 168), 1: 111379\n",
      "(96, 168), 2: 71599\n",
      "(96, 168), 3: 2111\n",
      "(104, 144), 1: 42279\n",
      "(104, 144), 2: 26993\n",
      "(104, 144), 3: 838\n",
      "(104, 152), 1: 1328367\n",
      "(104, 152), 2: 829638\n",
      "(104, 152), 3: 21897\n",
      "(112, 136), 1: 14660\n",
      "(112, 136), 2: 9677\n",
      "(112, 136), 3: 255\n",
      "(112, 144), 1: 260286\n",
      "(112, 144), 2: 165814\n",
      "(112, 144), 3: 4543\n",
      "(120, 128), 1: 9198\n",
      "(120, 128), 2: 6372\n",
      "(120, 128), 3: 175\n",
      "(120, 136), 1: 10361\n",
      "(120, 136), 2: 6685\n",
      "(120, 136), 3: 183\n",
      "(128, 120), 1: 10389\n",
      "(128, 120), 2: 7273\n",
      "(128, 120), 3: 224\n",
      "(128, 128), 1: 60169\n",
      "(128, 128), 2: 40974\n",
      "(128, 128), 3: 1231\n",
      "(136, 112), 1: 23653\n",
      "(136, 112), 2: 16488\n",
      "(136, 112), 3: 472\n",
      "(136, 120), 1: 15160\n",
      "(136, 120), 2: 10837\n",
      "(136, 120), 3: 306\n",
      "(144, 104), 1: 447138\n",
      "(144, 104), 2: 306036\n",
      "(144, 104), 3: 8975\n",
      "(144, 112), 1: 117032\n",
      "(144, 112), 2: 79481\n",
      "(144, 112), 3: 2307\n",
      "(152, 104), 1: 1357564\n",
      "(152, 104), 2: 850273\n",
      "(152, 104), 3: 21450\n",
      "(160, 96), 1: 24705\n",
      "(160, 96), 2: 17136\n",
      "(160, 96), 3: 504\n",
      "(168, 88), 1: 17692\n",
      "(168, 88), 2: 13053\n",
      "(168, 88), 3: 418\n",
      "(168, 96), 1: 84312\n",
      "(168, 96), 2: 59845\n",
      "(168, 96), 3: 1832\n",
      "(176, 88), 1: 7997\n",
      "(176, 88), 2: 5991\n",
      "(176, 88), 3: 200\n",
      "(184, 88), 1: 15976\n",
      "(184, 88), 2: 11062\n",
      "(184, 88), 3: 327\n",
      "(192, 80), 1: 1085\n",
      "(192, 80), 2: 725\n",
      "(192, 80), 3: 17\n",
      "(200, 80), 1: 369\n",
      "(200, 80), 2: 254\n",
      "(200, 80), 3: 12\n",
      "(208, 72), 1: 254\n",
      "(208, 72), 2: 197\n",
      "(208, 72), 3: 5\n",
      "(216, 72), 1: 223\n",
      "(216, 72), 2: 110\n",
      "(216, 72), 3: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "base_revision = \"462165984030d82259a11f4367a4eed129e94a7b\"\n",
    "device_batch_size = 16\n",
    "num_workers = 2\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"vae\", revision=\"462165984030d82259a11f4367a4eed129e94a7b\", torch_dtype=torch.float32, use_safetensors=True)\n",
    "assert isinstance(vae, AutoencoderKL)\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(base_model, subfolder=\"tokenizer\", revision=base_revision, use_fast=False)\n",
    "tokenizer_2 = CLIPTokenizer.from_pretrained(base_model, subfolder=\"tokenizer_2\", revision=base_revision, use_fast=False)\n",
    "source_ds = datasets.load_dataset(\"parquet\", data_files=\"../data/dataset.parquet\")\n",
    "assert isinstance(source_ds, datasets.DatasetDict)\n",
    "source_ds = source_ds['train'].train_test_split(test_size=2048, seed=42)\n",
    "\n",
    "train_ds = ImageDatasetPrecoded(source_ds['train'], tokenizer, tokenizer_2, datapath='../data/vaes')\n",
    "test_ds = ImageDatasetPrecoded(source_ds['test'], tokenizer, tokenizer_2, datapath='../data/vaes')\n",
    "\n",
    "train_buckets = gen_buckets(source_ds['train'])\n",
    "validation_buckets = gen_buckets(source_ds['test'])\n",
    "\n",
    "print(\"Aspect ratio buckets:\")\n",
    "#sorted_buckets = sorted(train_buckets, key=lambda x: (x.resolution, x.n_chunks))\n",
    "for bucket in sorted(train_buckets, key=lambda x: (x.resolution, x.n_chunks)):\n",
    "\tprint(f\"{bucket.resolution}, {bucket.n_chunks}: {len(bucket.images)}\")\n",
    "print()\n",
    "\n",
    "\n",
    "train_sampler = AspectBucketSampler(dataset=train_ds, buckets=train_buckets, batch_size=device_batch_size, num_replicas=1, rank=0, shuffle=True, ragged_batches=False)\n",
    "validation_sampler = AspectBucketSampler(dataset=test_ds, buckets=validation_buckets, batch_size=device_batch_size, num_replicas=1, rank=0, shuffle=False, ragged_batches=True)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "\ttrain_ds,\n",
    "\tbatch_sampler=train_sampler,\n",
    "\tnum_workers=num_workers,\n",
    "\tcollate_fn=train_ds.collate_fn,\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "\ttest_ds,\n",
    "\tbatch_sampler=validation_sampler,\n",
    "\tnum_workers=num_workers,\n",
    "\tcollate_fn=test_ds.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832x1216: 2229287\n",
      "1216x832: 2179902\n",
      "832x1152: 762149\n",
      "1152x896: 430643\n",
      "896x1152: 198820\n",
      "1344x768: 185089\n",
      "768x1344: 145989\n",
      "1024x1024: 102374\n",
      "1152x832: 70110\n",
      "1280x768: 58728\n",
      "768x1280: 42345\n",
      "896x1088: 40613\n",
      "1344x704: 31708\n",
      "704x1344: 31163\n",
      "704x1472: 27365\n",
      "960x1088: 26303\n",
      "1088x896: 24592\n",
      "1472x704: 17991\n",
      "960x1024: 17886\n",
      "1088x960: 17229\n",
      "1536x640: 16485\n",
      "1024x960: 15745\n",
      "704x1408: 14188\n",
      "1408x704: 12204\n",
      "1600x640: 4835\n",
      "1728x576: 4718\n",
      "1664x576: 2999\n",
      "640x1536: 1827\n",
      "640x1600: 635\n",
      "576x1664: 456\n",
      "576x1728: 335\n"
     ]
    }
   ],
   "source": [
    "foo_res = defaultdict(int)\n",
    "\n",
    "for bucket in sorted(train_buckets, key=lambda x: (x.resolution, x.n_chunks)):\n",
    "\twidth = bucket.resolution[1] * 8\n",
    "\theight = bucket.resolution[0] * 8\n",
    "\tfoo_res[(width, height)] += len(bucket.images)\n",
    "\n",
    "for resolution, count in sorted(foo_res.items(), key=lambda x: x[1], reverse=True):\n",
    "\tprint(f\"{resolution[0]}x{resolution[1]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:13<00:00, 7273.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 21.3453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22372/100000 [00:04<00:15, 4894.64it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (82 > 77). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 100000/100000 [00:20<00:00, 4966.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 39.53047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:27<00:00, 3650.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 61.55775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:35<00:00, 2840.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 83.5767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "IMPORTANT_TAGS = set(['watermark'])\n",
    "\n",
    "\n",
    "def build_prompt_from_tags(tag_string: str, n_tags: int) -> str:\n",
    "\t# Split tag string into tags\n",
    "\t# Tags are shuffled, important tags are always included, and the number of tags is limited by n_tags\n",
    "\ttags = set(tag.strip() for tag in tag_string.split(\",\") if tag.strip())\n",
    "\timportant_tags = tags.intersection(IMPORTANT_TAGS)\n",
    "\tn_tags = min(max(n_tags, len(important_tags)), len(tags))\n",
    "\ttags = list(important_tags) + random.sample(list(tags - important_tags), n_tags - len(important_tags))\n",
    "\tassert len(tags) <= n_tags, f\"Expected {n_tags} tags, got {len(tags)}\"\n",
    "\trandom.shuffle(tags)\n",
    "\n",
    "\t# Prompt construction\n",
    "\ttag_type = random.randint(0, 2)   # Use underscores, spaces, or mixed\n",
    "\n",
    "\tprompt = \"\"\n",
    "\tfor tag in tags:\n",
    "\t\t# Regularize across tags with spaces or underscores, or mixed.\n",
    "\t\tif tag_type == 1:\n",
    "\t\t\ttag = tag.replace(\"_\", \" \")\n",
    "\t\telif tag_type == 2:\n",
    "\t\t\tif random.random() < 0.8:\n",
    "\t\t\t\ttag = tag.replace(\"_\", \" \")\n",
    "\t\t\n",
    "\t\tif len(prompt) > 0:\n",
    "\t\t\tprompt += \",\"\n",
    "\t\t\t# Space between most times\n",
    "\t\t\t# NOTE: I don't think this matters because CLIP tokenizer ignores spaces?\n",
    "\t\t\tif random.random() < 0.8:\n",
    "\t\t\t\tprompt += ' '\n",
    "\t\t\tprompt += tag\n",
    "\t\telse:\n",
    "\t\t\tprompt += tag\n",
    "\t\t\t\n",
    "\treturn prompt\n",
    "\n",
    "\n",
    "tag_strings = list(source_ds['train']['tag_string'])\n",
    "random.shuffle(tag_strings)\n",
    "tag_strings = tag_strings[:100000]\n",
    "\n",
    "for n_tags in [8, 16, 32, 64]:\n",
    "\tcounts = []\n",
    "\tfor row in tqdm(tag_strings):\n",
    "\t\tprompt = build_prompt_from_tags(row, n_tags)\n",
    "\t\tfoo = tokenizer.encode(prompt, add_special_tokens=False, padding=False)\n",
    "\t\tcounts.append(len(foo))\n",
    "\n",
    "\tprint(n_tags, sum(counts) / len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing indexes in training batches: 682\n",
      "Number of missing indexes in validation batches (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "indexes = defaultdict(int)\n",
    "for batch in iter(train_sampler):\n",
    "\tfor item in batch:\n",
    "\t\tindexes[item[1]] += 1\n",
    "\n",
    "for key, value in indexes.items():\n",
    "\tif value > 1:\n",
    "\t\tprint(f\"Index {key} was sampled {value} times\")\n",
    "\n",
    "missing_indexes = set(range(len(train_ds))) - set(indexes.keys())\n",
    "print(f\"Number of missing indexes in training batches: {len(missing_indexes)}\")\n",
    "\n",
    "\n",
    "\n",
    "indexes = defaultdict(int)\n",
    "for batch in iter(validation_sampler):\n",
    "\tfor item in batch:\n",
    "\t\tindexes[item[1]] += 1\n",
    "\n",
    "for key, value in indexes.items():\n",
    "\tif value > 1:\n",
    "\t\tprint(f\"Index {key} was sampled {value} times\")\n",
    "\n",
    "missing_indexes = set(range(len(test_ds))) - set(indexes.keys())\n",
    "print(f\"Number of missing indexes in validation batches (should be 0): {len(missing_indexes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10000/359050 [03:21<1:57:23, 49.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt lengths:\n",
      "2: 7127\n",
      "1: 2291\n",
      "3: 583\n",
      "\n",
      "Minimum original size: 527\n",
      "Minimum target size: 576\n"
     ]
    }
   ],
   "source": [
    "# Ensure that during training we see batches where the prompt is below 77 tokens\n",
    "# Otherwise the model might always expect long prompts.\n",
    "lengths = defaultdict(int)\n",
    "min_original = 9999999999\n",
    "min_target = 99999999999\n",
    "for batch in tqdm(iter(train_dataloader)):\n",
    "\tlength = batch['prompt'].shape[1]\n",
    "\tlengths[length] += 1\n",
    "\n",
    "\tmin_original = min(min_original, batch['original_size'].min().item())\n",
    "\tmin_target = min(min_target, batch['target_size'].min().item())\n",
    "\n",
    "\tif sum(lengths.values()) > 10000:\n",
    "\t\tbreak\n",
    "\n",
    "print(\"Prompt lengths:\")\n",
    "for length, count in lengths.items():\n",
    "\tprint(f\"{length}: {count}\")\n",
    "print()\n",
    "print(f\"Minimum original size: {min_original}\")\n",
    "print(f\"Minimum target size: {min_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler.set_epoch(random.randint(0, 1000000000))\n",
    "x = iter(train_dataloader)\n",
    "batch = next(x)\n",
    "batch = next(x)\n",
    "batch = next(x)\n",
    "batch = next(x)\n",
    "batch = next(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor i in range(len(batch['latent'])):\n",
    "\t\tlatent = batch['latent'][i]\n",
    "\t\toriginal_size = batch['original_size'][i]\n",
    "\t\ttarget_size = batch['target_size'][i]\n",
    "\t\tcrop = batch['crop'][i]\n",
    "\t\tprompt = batch['prompt'][i]\n",
    "\t\tprompt_2 = batch['prompt_2'][i]\n",
    "\n",
    "\t\t# Decode\n",
    "\t\tlatent = latent.float() / vae.config.scaling_factor\n",
    "\t\timage = vae.decode(latent.unsqueeze(0), return_dict=False)[0][0]\n",
    "\t\timage_pil = TVF.to_pil_image((image * 0.5 + 0.5).clamp(0, 1)).convert(\"RGB\")\n",
    "\n",
    "\t\tdisplay(image_pil)\n",
    "\t\tprint(f\"Original size (hxw): {original_size}\")\n",
    "\t\tprint(f\"Target size (hxw): {target_size}\")\n",
    "\t\tprint(f\"Crop (txl): {crop}\")\n",
    "\n",
    "\t\tfor line, line_2 in zip(prompt, prompt_2):\n",
    "\t\t\tprint(tokenizer.decode(line))\n",
    "\t\t\tprint(tokenizer_2.decode(line_2))\n",
    "\n",
    "\t\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmpenv5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
