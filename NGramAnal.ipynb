{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin\n",
    "import psycopg\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"n-grams.csv\", \"r\") as f:\n",
    "\treader = csv.reader(f)\n",
    "\tngrams = list(reader)\n",
    "\n",
    "# Filter\n",
    "header = ngrams.pop(0)\n",
    "ngrams = [ngram for ngram in ngrams if int(ngram[2]) > 1000]\n",
    "\n",
    "# Save\n",
    "with open(\"n-grams-filtered.csv\", \"w\") as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow((\"ngram\", \"text\", \"occurances\"))\n",
    "\twriter.writerows(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'and', '10532943']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading records: 6716761it [01:17, 86208.74it/s] \n"
     ]
    }
   ],
   "source": [
    "conn = psycopg.connect(dbname='postgres', user='postgres', host=str(Path.cwd() / \"pg-socket\"))\n",
    "\n",
    "with conn.cursor('dataset-builder') as cur:\n",
    "\tcur.execute(\"SELECT tag_string, subreddit, caption, caption_2, caption_3, caption_4, source FROM images WHERE embedding IS NOT NULL AND score IS NOT NULL AND score > 0 AND tag_string IS NOT NULL and caption IS NOT NULL\")\n",
    "\trecords = []\n",
    "\n",
    "\tfor tag_string, subreddit, caption, caption_2, caption_3, caption_4, source in tqdm(cur, desc=\"Reading records\", dynamic_ncols=True):\n",
    "\t\tcaption = caption\n",
    "\t\tif caption_2 is not None:\n",
    "\t\t\tcaption = caption_2\n",
    "\t\tif caption_3 is not None:\n",
    "\t\t\tcaption = caption_3\n",
    "\t\tif caption_4 is not None:\n",
    "\t\t\tcaption = caption_4\n",
    "\t\t\n",
    "\t\trecords.append((tag_string, subreddit, caption, source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tags: 100%|██████████| 6716761/6716761 [00:43<00:00, 155171.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tag_counts = defaultdict(int)\n",
    "VALID_SOURCES = {\"fansly\", \"flickr\", \"onlyfans\", \"unsplash\"}\n",
    "\n",
    "for tag_string, subreddit, caption, source in tqdm(records, desc=\"Counting tags\", dynamic_ncols=True):\n",
    "\ttags = tag_string.split(\",\")\n",
    "\tif subreddit is not None:\n",
    "\t\ttags.append(f\"r/{subreddit.lower()}\")\n",
    "\t\ttags.append(\"reddit\")\n",
    "\t\n",
    "\tif source is not None and source in VALID_SOURCES:\n",
    "\t\ttags.append(source)\n",
    "\t\n",
    "\tfor tag in tags:\n",
    "\t\ttag_counts[tag] += 1\n",
    "\n",
    "with open(\"tag-counts.csv\", \"w\") as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow((\"tag\", \"count\"))\n",
    "\tfor tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "\t\tif count < 1000:\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\twriter.writerow((tag, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_spacy_doc_info(doc: Doc):\n",
    "\t\"\"\"\n",
    "\tPrints comprehensive information for each token in a spaCy Doc object.\n",
    "\n",
    "\tParameters:\n",
    "\tdoc (spacy.tokens.Doc): A spaCy Doc object.\n",
    "\t\"\"\"\n",
    "\t# Print basic token-level information\n",
    "\tprint(\"Tokens and Attributes:\")\n",
    "\tprint(f\"{'Text':{10}} {'Lemma':{10}} {'POS':{6}} {'Tag':{6}} {'Dep':{10}} {'Shape':{8}} {'Is Alpha':{10}} {'Is Stop':{10}}\")\n",
    "\tprint(\"=\"*80)\n",
    "\n",
    "\tfor token in doc:\n",
    "\t\tprint(f\"{token.text:{10}} {token.lemma_:{10}} {token.pos_:{6}} {token.tag_:{6}} {token.dep_:{10}} {token.shape_:{8}} {str(token.is_alpha):{10}} {str(token.is_stop):{10}}\")\n",
    "\n",
    "\t# Print named entities\n",
    "\tprint(\"\\nNamed Entities:\")\n",
    "\tprint(f\"{'Entity':{20}} {'Label':{10}} {'Start':{6}} {'End':{6}}\")\n",
    "\tprint(\"=\"*50)\n",
    "\tfor ent in doc.ents:\n",
    "\t\tprint(f\"{ent.text:{20}} {ent.label_:{10}} {ent.start_char:{6}} {ent.end_char:{6}}\")\n",
    "\n",
    "\t# Print noun chunks\n",
    "\tprint(\"\\nNoun Chunks:\")\n",
    "\tprint(\"=\"*50)\n",
    "\tfor chunk in doc.noun_chunks:\n",
    "\t\tprint(f\"{chunk.text:{20}} - Root: {chunk.root.text} - Dep: {chunk.root.dep_} - Head: {chunk.root.head.text}\")\n",
    "\n",
    "\t# Print sentences\n",
    "\tprint(\"\\nSentences:\")\n",
    "\tprint(\"=\"*50)\n",
    "\tfor sent in doc.sents:\n",
    "\t\tprint(f\"Sentence: {sent.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for tag_string, subreddit, caption, source in tqdm(records, desc=\"Processing records\", dynamic_ncols=True):\n",
    "\tdoc = nlp(caption)\n",
    "\n",
    "\tprint_spacy_doc_info(doc)\n",
    "\n",
    "\tprint(\"\\n\" * 2)\n",
    "\n",
    "\tnlp = spacy.load(\"en_core_web_md\")\n",
    "\tdoc = nlp(caption)\n",
    "\tprint_spacy_doc_info(doc)\n",
    "\n",
    "\tprint(\"\\n\" * 2)\n",
    "\n",
    "\tnlp = spacy.load(\"en_core_web_lg\")\n",
    "\tdoc = nlp(caption)\n",
    "\tprint_spacy_doc_info(doc)\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6716761/6716761 [00:01<00:00, 4491377.36it/s]\n"
     ]
    }
   ],
   "source": [
    "captions = [caption for tag_string, subreddit, caption, source in tqdm(records)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing captions: 100%|██████████| 6716761/6716761 [57:19<00:00, 1953.01it/s]  \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "docs = list(tqdm(nlp.pipe(captions, batch_size=256, n_process=16), desc=\"Processing captions\", total=len(captions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6717/6717 [55:23<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(docs), 1000)):\n",
    "\tdoc_bin = DocBin(store_user_data=True)\n",
    "\tfor doc in docs[i:i+1000]:\n",
    "\t\tdoc_bin.add(doc)\n",
    "\tdst = Path.cwd() / \"spacy-captions\" / f\"captions-{i}.spacy\"\n",
    "\tdst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\tdoc_bin.to_disk(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_noun_phrases(doc: Doc) -> list[str]:\n",
    "\tmerged_tokens = []\n",
    "\tstart = 0\n",
    "\n",
    "\tfor chunk in doc.noun_chunks:\n",
    "\t\tmerged_tokens.extend([token.text for token in doc[start:chunk.start] if not token.is_punct])\n",
    "\t\tmerged_tokens.append(chunk.text)\n",
    "\t\tstart = chunk.end\n",
    "\t\n",
    "\tmerged_tokens.extend([token.text for token in doc[start:] if not token.is_punct])\n",
    "\n",
    "\treturn merged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def parse_docs(doc_path: Path) -> dict[int, dict[str, int]]:\n",
    "\tn_grams = {n: defaultdict(int) for n in range(1, 8)}\n",
    "\tnlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\tdoc_bin = DocBin().from_disk(doc_path)\n",
    "\tdocs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "\tfor doc in docs:\n",
    "\t\ttokens = merge_noun_phrases(doc)\n",
    "\n",
    "\t\tfor n in n_grams.keys():\n",
    "\t\t\tfor i in range(len(tokens) - n + 1):\n",
    "\t\t\t\tn_gram = \" \".join([tokens[j].lower() for j in range(i, i + n)])\n",
    "\t\t\t\tn_gram = n_gram.strip()\n",
    "\t\t\t\tn_grams[n][n_gram] += 1\n",
    "\t\n",
    "\treturn n_grams\n",
    "\n",
    "\n",
    "all_n_grams = {n: defaultdict(int) for n in range(1, 8)}\n",
    "\n",
    "saved_docs = list((Path.cwd() / \"spacy-captions\").glob(\"*.spacy\"))\n",
    "\n",
    "with Pool(16) as pool:\n",
    "\tfor result in tqdm(pool.imap_unordered(parse_docs, saved_docs), total=len(saved_docs), desc=\"Reading captions\", dynamic_ncols=True):\n",
    "\t\tfor n, n_grams in result.items():\n",
    "\t\t\tfor n_gram, count in n_grams.items():\n",
    "\t\t\t\tall_n_grams[n][n_gram] += count\n",
    "\n",
    "\n",
    "for n, n_gram in all_n_grams.items():\n",
    "\tprint(f\"{n}-grams:\")\n",
    "\tfor n_gram, count in sorted(n_gram.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "\t\tprint(f\"{n_gram}: {count}\")\n",
    "\t\n",
    "\tprint(\"\\n\" * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:05<00:00, 18.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# Save the n-grams to a csv file\n",
    "rows = []\n",
    "for n, n_gram in tqdm(all_n_grams.items()):\n",
    "\tfor n_gram, count in sorted(n_gram.items(), key=lambda x: x[1], reverse=True)[:10000]:\n",
    "\t\trows.append((n, n_gram, count))\n",
    "\n",
    "with open(\"n-grams.csv\", \"w\") as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow([\"n\", \"n-gram\", \"count\"])\n",
    "\twriter.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = {n: defaultdict(int) for n in range(1, 8)}\n",
    "gc.collect()\n",
    "\n",
    "for doc in tqdm(docs, desc=\"Processing captions\", dynamic_ncols=True):\n",
    "\ttokens = merge_noun_phrases(doc)\n",
    "\n",
    "\tfor n in n_grams.keys():\n",
    "\t\tfor i in range(len(tokens) - n + 1):\n",
    "\t\t\tn_gram = \" \".join([tokens[j].lower() for j in range(i, i + n)])\n",
    "\t\t\tn_grams[n][n_gram] += 1\n",
    "\n",
    "for n, n_gram in n_grams.items():\n",
    "\tprint(f\"{n}-grams:\")\n",
    "\tfor n_gram, count in sorted(n_gram.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "\t\tprint(f\"{n_gram}: {count}\")\n",
    "\t\n",
    "\tprint(\"\\n\" * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:11<00:00, 18.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# Save the n-grams to a file\n",
    "with open(\"n-grams.txt\", \"w\") as f:\n",
    "\tfor n, n_gram in tqdm(n_grams.items()):\n",
    "\t\tfor n_gram, count in sorted(n_gram.items(), key=lambda x: x[1], reverse=True)[:10000]:\n",
    "\t\t\tf.write(f\"{n},{n_gram},{count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = {n: defaultdict(int) for n in range(1, 8)}\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "\ttokens = [token for token in doc if not token.is_punct]\n",
    "\n",
    "\tfor n in n_grams.keys():\n",
    "\t\tfor i in range(len(tokens) - n + 1):\n",
    "\t\t\tn_gram = \" \".join([tokens[j].text.lower() for j in range(i, i + n)])\n",
    "\t\t\tn_grams[n][n_gram] += 1\n",
    "\n",
    "for n, n_gram in n_grams.items():\n",
    "\tprint(f\"{n}-grams:\")\n",
    "\tfor n_gram, count in sorted(n_gram.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "\t\tprint(f\"{n_gram}: {count}\")\n",
    "\t\n",
    "\tprint(\"\\n\" * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmpenv5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
